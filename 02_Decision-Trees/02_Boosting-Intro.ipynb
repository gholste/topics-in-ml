{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting<br><sub>Author: Greg Holste<br></sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting is an **ensemble** technique for classification or regression; ensemble methods in general work by fitting many models to the data and somehow aggregating their predictions. The broad idea behind boosting is to sequentially train many *weak learners* (models barely better than random guessing), adaptively giving more weight to misclassified examples at each iteration. This way, the boosted model learns to \"pay more attention\" to the hardest-to-learn training observations. The learners in a boosted model are typically classification/regression trees, but in principle boosting can be applied to any model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost.M1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first boosting algorithm for binary classification. Let our training set consist of $n$ pairs $\\{({\\bf x_i}, y_i)\\}_{i=1}^n$, where each ${\\bf x_i}$ is a vector of (continuous or discrete) features and each $y_i \\in \\{-1, 1\\}$ is a binary label. For convenience, $I(\\cdot) = \\begin{cases} 1, \\cdot \\text{ is true}\\\\ 0, \\cdot \\text{ is false}\\end{cases}$ is the *indicator function*. The algorithm is as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initialize $w_i \\gets \\frac{1}{n}$ for $i = 1, \\dots, n$\n",
    "2. For $m = 1, \\dots, M$ do\n",
    "    1. Fit classifier $G_m({\\textbf x})$ to data with weights $w_i, i = 1, \\dots, n$\n",
    "    2. Find weighted error $err_m = \\frac{\\sum_{i=1}^n w_i I(y_i \\neq G_m({\\bf x}))}{\\sum_{i=1}^n w_i}$\n",
    "    3. Find $\\alpha_m = log\\left(\\frac{1 - err_m}{err_m}\\right)$\n",
    "    4. Update weights via $w_i \\gets w_i \\cdot e^{\\alpha_m I(y_i \\neq G_m({\\bf x})}$\n",
    "3. Output boosted classifier $G({\\bf x}) = sign\\{\\sum_{m=1}^M \\alpha_m G_m({\\bf x})\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we give equal weight to each training observation and train a classifier. We then find a weighted measure of training error for the classifier $G_m({\\bf x})$ (at first, $err_m$ is simply the misclassification rate). In step 2D, we now update update the weights for *only the misclassified observations*; that is, **each misclassified observation has its weight scaled by $e^{\\alpha_m}$, thus increasing its influence on the next classifier**. After repeating this process $M$ times, we finally output a weighted sum of our $M$ weak learners, where the weights $\\alpha_m, m = 1, \\dots, M$ correspond to the predictive ability of each learner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to apply AdaBoost with classification trees. Then two basic hyperparameters of interest are $M$, the number of learners in our ensemble, and $J_m, m=1,\\dots,M$, the \"depth\" of each tree. Interestingly, often shallower trees see better overall performance; using $J_m=1$ for all $m=1,\\dots,M$ -- creating an ensemble of \"stumps\" (trees with a single decision node) -- is often a good starting point. That being said, $M$ and $J_m$ should be tuned like any other hyperparameter via *cross-validation* or a related method. As a final note, AdaBoost can be expanded to accomodate multiple output classes and even probabilities of class membership. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boosting provides a powerful, generalized approach to ensembling learners, most often decision trees. First observe that we can formally represent a decision tree as $T({\\bf x}) = \\sum_{j=1}^J \\gamma_j I({\\bf x} \\in R_j)$, where $\\gamma_j$ is the constant predicted value associated with terminal node region $R_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initialize $f_0({\\bf x}) = \\arg\\!\\min_{\\gamma} \\sum_{i=1}^n L(y_i, \\gamma)$\n",
    "2. For $m = 1, \\dots, M$ do\n",
    "    1. Find $r_{im} = -\\left[\\frac{\\partial L(y_i, f_m({\\bf x_i}))}{\\partial f_m({\\bf x_i})}\\right]$ for all $i = 1, \\dots, n$\n",
    "    2. Fit regression tree to negative gradients $r_{im}$\n",
    "    3. Update $f_m({\\bf x}) \\gets f_{m-1}({\\bf x}) + \\lambda \\sum_{j=1} \\gamma_{jm} I({\\bf x} \\in R_{jm})$\n",
    "3. Output boosted model $\\hat{f}({\\bf x}) = f_M({\\bf x})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While may seem heavy on notation, the steps are actually quite simple. In step 1, we initalize our model with the best constant value (that which minimizes our loss). In step 2A, we find the negative gradient of our loss wrt each prediction. Observe that **when we use mean squared-error loss $L = -\\frac{1}{2}(y_i - f({\\bf x_i}))^2$, each $r_{im}$ is simply the *residual* $y_i - f({\\bf x_i})$**; for this reason, these $r_{im}, i = 1, \\dots, n$ are aptly called \"pseudoresiduals.\" We then fit a regression tree using these psuedoresiduals as labels (instead of our true training labels); in principle, we can fit any model to our negative gradients -- not just a decision tree. We then add a shrunken version of this tree (shrunk by learning rate $\\lambda$) to $f_0({\\bf x})$ and repeat this process $M$ times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to use this algorithm for a classification in which our output has $C$ unordered levels. Then we could use the multinomial deviance loss $L = -\\sum_{c \\in C} I(y_i = c)\\ log(p_c({\\bf x}))$, where $p_c({\\bf x}) = \\frac{e^{f_c({\\bf x})}}{\\sum_{l \\in C} e^{f_l({\\bf x})}}$ is the *softmax* function which maps inputs to probabilities (that sum to $1$). We would then repeat steps 2A-2C $C$ times -- where our target is an indicator for each level of output $y$ -- and proceed as normal, noting that the negative gradients in Step 2A are given by $r_{imc} = I(y_i = c) - p_c({\\bf x_i})$ for $i = 1, \\dots, n$ and $c \\in C$.\n",
    "\n",
    "Now we have an additional hyperparameter $\\lambda$ to consider; there is an inherent trade-off between the number of learners $M$ and the learning rate (or \"step size\") $\\lambda$, and these should be tuned accordingly. As a final note, observe that gradient boosted regression trees (GBRT) is more general than AdaBoost because it can do regression *or* classification and can accomodate any loss function. Furthermore, when using GBRT to perform binary classification with an exponential loss function, this algorithm reduces to AdaBoost.M1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost: Toy Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a simple application of these boosting methods to a toy binary classification problem. We will show that boosting outperforms a single tree and that our implementation of AdaBoost (in `boosting.py`) matches that of sk-learn. Here is our data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>$X_1$</th>\n",
       "      <th>$X_2$</th>\n",
       "      <th>$X_3$</th>\n",
       "      <th>$X_4$</th>\n",
       "      <th>$Y$</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-1.668532</td>\n",
       "      <td>-1.299013</td>\n",
       "      <td>0.274647</td>\n",
       "      <td>-0.603620</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-2.972883</td>\n",
       "      <td>-1.088783</td>\n",
       "      <td>0.708860</td>\n",
       "      <td>0.422819</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.596141</td>\n",
       "      <td>-1.370070</td>\n",
       "      <td>-3.116857</td>\n",
       "      <td>0.644452</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-1.068947</td>\n",
       "      <td>-1.175057</td>\n",
       "      <td>-1.913743</td>\n",
       "      <td>0.663562</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-1.305269</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>-0.154072</td>\n",
       "      <td>1.193612</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>995</td>\n",
       "      <td>-0.383660</td>\n",
       "      <td>0.952012</td>\n",
       "      <td>-1.738332</td>\n",
       "      <td>0.707135</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>996</td>\n",
       "      <td>-0.120513</td>\n",
       "      <td>1.172387</td>\n",
       "      <td>0.030386</td>\n",
       "      <td>0.765002</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>997</td>\n",
       "      <td>0.917112</td>\n",
       "      <td>1.105966</td>\n",
       "      <td>0.867665</td>\n",
       "      <td>-2.256250</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>998</td>\n",
       "      <td>0.100277</td>\n",
       "      <td>1.458758</td>\n",
       "      <td>-0.443603</td>\n",
       "      <td>-0.670023</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>999</td>\n",
       "      <td>1.041523</td>\n",
       "      <td>-0.019871</td>\n",
       "      <td>0.152164</td>\n",
       "      <td>-1.940533</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        $X_1$     $X_2$     $X_3$     $X_4$  $Y$\n",
       "0   -1.668532 -1.299013  0.274647 -0.603620 -1.0\n",
       "1   -2.972883 -1.088783  0.708860  0.422819 -1.0\n",
       "2   -0.596141 -1.370070 -3.116857  0.644452 -1.0\n",
       "3   -1.068947 -1.175057 -1.913743  0.663562 -1.0\n",
       "4   -1.305269 -0.965926 -0.154072  1.193612 -1.0\n",
       "..        ...       ...       ...       ...  ...\n",
       "995 -0.383660  0.952012 -1.738332  0.707135  1.0\n",
       "996 -0.120513  1.172387  0.030386  0.765002  1.0\n",
       "997  0.917112  1.105966  0.867665 -2.256250  1.0\n",
       "998  0.100277  1.458758 -0.443603 -0.670023  1.0\n",
       "999  1.041523 -0.019871  0.152164 -1.940533  1.0\n",
       "\n",
       "[1000 rows x 5 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from boosting import *\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification, load_boston\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=4,\n",
    "                           n_informative=2, n_redundant=0,\n",
    "                           random_state=0, shuffle=False)\n",
    "y[y == 0] = -1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "df = pd.DataFrame(np.concatenate([X, y[:, np.newaxis]], axis=-1))\n",
    "df.columns = ['$X_1$', '$X_2$', '$X_3$', '$X_4$', '$Y$']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first look at results when fitting a single classification tree to the training data with minimum terminal node size $2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.133% training accuracy\n",
      "93.2% test accuracy\n"
     ]
    }
   ],
   "source": [
    "DT = DecisionTreeClassifier(min_samples_leaf=2).fit(X_train, y_train)\n",
    "print(f\"{round(DT.score(X_train, y_train) * 100, 3)}% training accuracy\")\n",
    "print(f\"{round(DT.score(X_test, y_test) * 100, 3)}% test accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use AdaBoost with $M=500$ and $J_m=1$ for all $m=1,\\dots,m$ (comparing our custom implementation with sklearn's). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.133% training accuracy with sk-learn AdaBoost\n",
      "98.133% training accuracy with custom AdaBoost\n",
      "95.6% test accuracy with sk-learn AdaBoost\n",
      "95.6% test accuracy with custom AdaBoost\n"
     ]
    }
   ],
   "source": [
    "AB = AdaBoostClassifier(n_estimators=500, algorithm='SAMME').fit(X_train, y_train)\n",
    "\n",
    "AB2 = AdaBoost(X_train, y_train, M=500, J=1)\n",
    "AB2.fit()\n",
    "\n",
    "print(f\"{round(AB.score(X_train, y_train) * 100, 3)}% training accuracy with sk-learn AdaBoost\")\n",
    "print(f\"{round(acc(y_train, AB2.predict(X_train)) * 100, 3)}% training accuracy with custom AdaBoost\")\n",
    "print(f\"{round(AB.score(X_test, y_test) * 100, 3)}% test accuracy with sk-learn AdaBoost\")\n",
    "print(f\"{round(acc(y_test, AB2.predict(X_test)) * 100, 3)}% test accuracy with custom AdaBoost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBRT: Toy Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's consider a quick example applying gradient boosted regression trees (GBRT) to the Boston house pricing dataset (from the UCI Machine Learning Repository)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>$X_{1}$</th>\n",
       "      <th>$X_{2}$</th>\n",
       "      <th>$X_{3}$</th>\n",
       "      <th>$X_{4}$</th>\n",
       "      <th>$X_{5}$</th>\n",
       "      <th>$X_{6}$</th>\n",
       "      <th>$X_{7}$</th>\n",
       "      <th>$X_{8}$</th>\n",
       "      <th>$X_{9}$</th>\n",
       "      <th>$X_{10}$</th>\n",
       "      <th>$X_{11}$</th>\n",
       "      <th>$X_{12}$</th>\n",
       "      <th>$X_{13}$</th>\n",
       "      <th>$Y$</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>501</td>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "      <td>22.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>502</td>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>503</td>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "      <td>23.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>504</td>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>505</td>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     $X_{1}$  $X_{2}$  $X_{3}$  $X_{4}$  $X_{5}$  $X_{6}$  $X_{7}$  $X_{8}$  \\\n",
       "0    0.00632     18.0     2.31      0.0    0.538    6.575     65.2   4.0900   \n",
       "1    0.02731      0.0     7.07      0.0    0.469    6.421     78.9   4.9671   \n",
       "2    0.02729      0.0     7.07      0.0    0.469    7.185     61.1   4.9671   \n",
       "3    0.03237      0.0     2.18      0.0    0.458    6.998     45.8   6.0622   \n",
       "4    0.06905      0.0     2.18      0.0    0.458    7.147     54.2   6.0622   \n",
       "..       ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "501  0.06263      0.0    11.93      0.0    0.573    6.593     69.1   2.4786   \n",
       "502  0.04527      0.0    11.93      0.0    0.573    6.120     76.7   2.2875   \n",
       "503  0.06076      0.0    11.93      0.0    0.573    6.976     91.0   2.1675   \n",
       "504  0.10959      0.0    11.93      0.0    0.573    6.794     89.3   2.3889   \n",
       "505  0.04741      0.0    11.93      0.0    0.573    6.030     80.8   2.5050   \n",
       "\n",
       "     $X_{9}$  $X_{10}$  $X_{11}$  $X_{12}$  $X_{13}$   $Y$  \n",
       "0        1.0     296.0      15.3    396.90      4.98  24.0  \n",
       "1        2.0     242.0      17.8    396.90      9.14  21.6  \n",
       "2        2.0     242.0      17.8    392.83      4.03  34.7  \n",
       "3        3.0     222.0      18.7    394.63      2.94  33.4  \n",
       "4        3.0     222.0      18.7    396.90      5.33  36.2  \n",
       "..       ...       ...       ...       ...       ...   ...  \n",
       "501      1.0     273.0      21.0    391.99      9.67  22.4  \n",
       "502      1.0     273.0      21.0    396.90      9.08  20.6  \n",
       "503      1.0     273.0      21.0    396.90      5.64  23.9  \n",
       "504      1.0     273.0      21.0    393.45      6.48  22.0  \n",
       "505      1.0     273.0      21.0    396.90      7.88  11.9  \n",
       "\n",
       "[506 rows x 14 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "df = pd.DataFrame(np.concatenate([X, y[:, np.newaxis]], axis=-1))\n",
    "df.columns = [\"$X_{\" + str(i) + \"}$\" for i in range(1, X.shape[1] + 1)] + [\"$Y$\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's apply a single decision tree with mean squared-error (MSE) loss and minimum node size $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 = 1.0 on training set\n",
      "R^2 = 0.613 on test set\n"
     ]
    }
   ],
   "source": [
    "DT = DecisionTreeRegressor(min_samples_leaf=1, random_state=0).fit(X_train, y_train)\n",
    "print(f\"R^2 = {round(DT.score(X_train, y_train), 3)} on training set\")\n",
    "print(f\"R^2 = {round(DT.score(X_test, y_test), 3)} on test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us use GBRT with $M=1000$, $J_m=2, i=1,\\dots,M$, and $\\lambda = 0.1$, again comparing our implementation with that of sklearn. (Note: While there is no randomness in the GBRT algorithm as presented here, there could be slight differences in performance due to lack of precision (when summing the many small gradients).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 = 0.998 on training set\n",
      "R^2 = 0.998 on training set\n",
      "R^2 = 0.693 on test set\n",
      "R^2 = 0.693 on test set\n"
     ]
    }
   ],
   "source": [
    "GB = GradientBoostingRegressor(n_estimators=1000, max_depth=2, learning_rate=0.1, criterion='mse',\n",
    "                               init=DecisionTreeRegressor(max_depth=1).fit(X_train, y_train))\n",
    "GB = GB.fit(X_train, y_train)\n",
    "\n",
    "GB2 = GradientBoostRegressor(X_train, y_train, M=1000, J=2, lam=0.1)\n",
    "GB2.fit()\n",
    "\n",
    "print(f\"R^2 = {round(GB.score(X_train, y_train), 3)} on training set\")\n",
    "print(f\"R^2 = {round(R2(y_train, GB2.predict(X_train)), 3)} on training set\")\n",
    "print(f\"R^2 = {round(GB.score(X_test, y_test), 3)} on test set\")\n",
    "print(f\"R^2 = {round(R2(y_test, GB2.predict(X_test)), 3)} on test set\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
